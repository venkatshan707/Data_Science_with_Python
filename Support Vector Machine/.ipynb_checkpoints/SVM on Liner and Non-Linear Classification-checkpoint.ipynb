{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clssification using SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Suppose you are given plot of two label classes on graph as shown in image (A). Can you decide a separating line for the classes?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![image](https://www.linkpicture.com/q/SVM1.png)](https://www.linkpicture.com/view.php?img=LPic62fa32c51e305162112667)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might have come up with something similar to following image (image B). It fairly separates the two classes. Any point that is left of line falls into black circle class and on right falls into blue square class. Separation of classes. That’s what SVM does. It finds out a line/ hyper-plane (in multidimensional space that separate outs classes)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![image](https://www.linkpicture.com/q/SVM2.png)](https://www.linkpicture.com/view.php?img=LPic62fa32c51e305162112667)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making it a Bit complex…\n",
    "-------------------------\n",
    "So far so good. Now consider what if we had data as shown in image below? Clearly, there is no line that can separate the two classes in this x-y plane. So what do we do? We apply transformation and add one more dimension as we call it z-axis. Lets assume value of points on z plane, w = x² + y². In this case we can manipulate it as distance of point from z-origin. \n",
    "\n",
    "Now if we plot in z-axis, a clear separation is visible \n",
    "and a line can be drawn ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![image](https://www.linkpicture.com/q/SVM3.png)](https://www.linkpicture.com/view.php?img=LPic62fa32c51e305162112667)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "[![image](https://www.linkpicture.com/q/SVM4.png)](https://www.linkpicture.com/view.php?img=LPic62fa32c51e305162112667)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we transform back this line to original plane, it maps to circular boundary as shown in image E. These transformations are called kernels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![image](https://www.linkpicture.com/q/SVM5.png)](https://www.linkpicture.com/view.php?img=LPic62fa32c51e305162112667)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making it a little more complex…\n",
    "-----------------------------------------------\n",
    "\n",
    "What if data plot overlaps? \n",
    "Or, \n",
    "what in case some of the black points are inside the blue ones? \n",
    "Which line among 1 or 2 ? should we draw ?\n",
    "\n",
    "[![image](https://www.linkpicture.com/q/SVM6.png)](https://www.linkpicture.com/view.php?img=LPic62fa348765b821502125874)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is called regularization parameter. \n",
    "\n",
    "* In next section, we define two terms regularization parameter and gamma. \n",
    "\n",
    "** These are tuning parameters in SVM classifier. Varying those we can achive considerable non linear classification line with more accuracy in reasonable amount of time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tuning parameters: Regularization, Gamma and Margin.\n",
    "--------------------------------------------------------------------------------\n",
    "\n",
    "Regularization :\n",
    "---------------------\n",
    "The Regularization parameter (often termed as C parameter in python’s sklearn library) tells the SVM optimization how much you want to avoid misclassifying each training example.\n",
    "\n",
    "For large values of C(right diagram), the optimization will choose a smaller-margin hyperplane if that hyperplane does a better job of getting all the training points classified correctly. Conversely, a very small value of C(left diagram) will cause the optimizer to look for a larger-margin separating hyperplane, even if that hyperplane misclassifies more points.\n",
    "\n",
    "The images below are example of two different regularization parameter. \n",
    "**Left one has some misclassification due to lower regularization value. \n",
    "**Higher value leads to results like right one.\n",
    "\n",
    "GOOGLE SAYS \"LIFE AND HAPPINESS\" IS THE MOST CRITICAL DATA. IF ANY THING OTHER THAN THAT COMES, WE CAN GO FOR LEFT DIAGRAM, ELSE WE SHOULD CUMPOLSORILY GO FOR RIGHT DIAGRAM\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![image](https://www.linkpicture.com/q/SVM7.png)](https://www.linkpicture.com/view.php?img=LPic62fa364fe1fa1389015983)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gamma\n",
    "-----------\n",
    "The gamma parameter defines how far the influence of a single training example reaches, with low values meaning ‘far’ and high values meaning ‘close’. In other words, with low gamma, points far away from plausible seperation line are considered in calculation for the seperation line. Where as high gamma means the points close to plausible line are considered in calculation.\n",
    "\n",
    "IF YOU HAVE TOO MUCH CONCENTRATION IN THE DATA, YOU USE HIGH GAMMA. EG: AVG MARKS OF 100 ARE MORE SATURATED IN REGION 50-75.\n",
    "\n",
    "IF YOU HAVE WELL DISTRIBUTED DATA(GAUSSIAN DISTRIBUTION), YOU USE LOW GAMMA. EG: AVG MARKS OF 100 ARE WELL DISTRIBUTED FROM 0 TO 100\n",
    "\n",
    "[![image](https://www.linkpicture.com/q/SVM8.png)](https://www.linkpicture.com/view.php?img=LPic62fa364fe1fa1389015983)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Margin\n",
    "----------\n",
    "\n",
    "A margin is a separation of line to the closest class points.\n",
    "\n",
    "A good margin is one where this separation is larger for both the classes. Images below gives to visual example of good and bad margin. A good margin allows the points to be in their respective classes without crossing to other class.\n",
    "\n",
    "INCASE OF BAD MARGIN, THE POINT WHICH IS VERY CLOSE TO THE MARGIN WILL GET UP MISCLASSIFIED WHEN THE NEW DATA COMES IN.\n",
    "\n",
    "[![image](https://www.linkpicture.com/q/SVM9.png)](https://www.linkpicture.com/view.php?img=LPic62fa379877673194483145)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM on Linear Classification Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1372, 5)\n",
      "------------\n",
      "   Variance  Skewness  Curtosis  Entropy  Class\n",
      "0   3.62160    8.6661   -2.8073 -0.44699      0\n",
      "1   4.54590    8.1674   -2.4586 -1.46210      0\n",
      "2   3.86600   -2.6383    1.9242  0.10645      0\n",
      "3   3.45660    9.5228   -4.0112 -3.59440      0\n",
      "4   0.32924   -4.4552    4.5718 -0.98880      0\n"
     ]
    }
   ],
   "source": [
    "#Doing the minimum necessary imports\n",
    "\n",
    "import pandas as pd  \n",
    "import numpy as np  \n",
    "import matplotlib.pyplot as plt  \n",
    "%matplotlib inline\n",
    "\n",
    "# reading data from CSV file. \n",
    "# reading bank currency note data into pandas dataframe.\n",
    "bankdata = pd.read_csv(\"https://raw.githubusercontent.com/venkatshan707/Data_Science_with_Python/main/Support%20Vector%20Machine/bill_authentication.csv\")  \n",
    "\n",
    "# Exploratory Data Analysis\n",
    "print(bankdata.shape)  \n",
    "print(\"------------\")\n",
    "print(bankdata.head()) \n",
    "#0=Fake Note, 1=Genuine Note"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    762\n",
       "1    610\n",
       "Name: Class, dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bankdata.Class.value_counts ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix: \n",
      "\n",
      " [[149   1]\n",
      " [  3 122]] \n",
      "Classification Report:                precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99       150\n",
      "           1       0.99      0.98      0.98       125\n",
      "\n",
      "    accuracy                           0.99       275\n",
      "   macro avg       0.99      0.98      0.99       275\n",
      "weighted avg       0.99      0.99      0.99       275\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "\n",
    "# Dividing data as Independat and Target Variables\n",
    "X = bankdata.drop('Class', axis=1)  \n",
    "y = bankdata['Class']  \n",
    "\n",
    "# Dividing data into training and test sets\n",
    "from sklearn.model_selection import train_test_split  \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20)\n",
    "\n",
    "\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html# Training the Algorithm. Here we would use simple SVM , i.e linear SVM\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# classifying linear data\n",
    "svclassifier = SVC(kernel= 'linear') # classify linear data\n",
    "#svclassifier = SVC() #bydefault, its rbf\n",
    "# kernel can take many values like\n",
    "# Gaussian, polynomial, sigmoid, or computable kernel\n",
    "\n",
    "# fit the model over data\n",
    "svclassifier.fit(X_train,y_train)\n",
    "\n",
    "\n",
    "# Making Predictions\n",
    "y_pred = svclassifier.predict(X_test)\n",
    "\n",
    "\n",
    "# Evaluating the Algorithm\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "print(\"Confusion Matrix: \\n\\n\", confusion_matrix(y_test,y_pred), \"\")\n",
    "print(\"Classification Report: \",classification_report(y_test,y_pred))\n",
    "\n",
    "\n",
    "# Remember : for evaluating classification-based ML algo use  \n",
    "# confusion_matrix, classification_report and accuracy_score.\n",
    "# And for evaluating regression-based ML Algo use Mean Squared Error(MSE), ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note : to understand Precision, recall, f1-score, support; see this post\n",
    "https://towardsdatascience.com/accuracy-precision-recall-or-f1-331fb37c5cb9\n",
    "\n",
    "For example : In the above o/p -> (refer confusion matrix)\n",
    "166/167 bank entries were correctly predicted false.\n",
    "also, 108/108 bank entries were correctly predicted true.\n",
    "\n",
    "The total no. of observations are also indicated as support. \n",
    "see support values -> for 0(i.e false) it is 167 and for 1(i.e true) it is 108 \n",
    "\n",
    "further, Precision talks about how precise/accurate your model is ?\n",
    "Precision tells us, out of those predicted positive, how many of them are actually positive. Our SVM model's precision is 1.00 i.e 100% in predicting the actual Negatives and 99% in predicting the actual positives. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Applying SVM over non-linear data\n",
    " \n",
    "In case of non-linearly separable data, the simple SVM algorithm cannot be used. Rather, a modified version of SVM, called Kernel SVM, is used.\n",
    "\n",
    "Basically, the kernel SVM projects the non-linearly separable data in lower dimensions to linearly separable data in higher dimensions in such a way that data points belonging to different classes are allocated to different dimensions. Again, there is complex mathematics involved in this, but you do not have to worry about it in order to use SVM. Rather we can simply use Python's Scikit-Learn library to implement and use the kernel SVM.\n",
    "\n",
    "Implementing Kernel SVM with Scikit-Learn is similar to the simple SVM. In this section, we will use the famous iris dataset to predict the category to which a plant belongs based on four attributes: sepal-width, sepal-length, petal-width and petal-length.\n",
    "\n",
    "We will try all three possible kernels; namely polynomial, Gaussian, and sigmoid kernels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal_length  sepal_width  petal_length  petal_width species\n",
       "0           5.1          3.5           1.4          0.2  setosa\n",
       "1           4.9          3.0           1.4          0.2  setosa\n",
       "2           4.7          3.2           1.3          0.2  setosa\n",
       "3           4.6          3.1           1.5          0.2  setosa\n",
       "4           5.0          3.6           1.4          0.2  setosa"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "from sklearn import svm, datasets\n",
    "# import some data to play with\n",
    "irisdata = sns.load_dataset('iris')\n",
    "irisdata.head()  # have a look at the attributres(=> X) and Labels(=> y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing data\n",
    "X = irisdata.drop('species', axis=1)  \n",
    "y = irisdata['species']\n",
    "\n",
    "# Train Test Split\n",
    "from sklearn.model_selection import train_test_split  \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Algorithm\n",
    "To train the kernel SVM, we use the same SVC class of the Scikit-Learn's svm library.\n",
    "\n",
    "We will implement polynomial, Gaussian, and sigmoid kernels to see which one works better for our problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Polynomial Kernel\n",
    "In the case of polynomial kernel, you also have to pass a value for the degree parameter of the SVC class. This basically is the degree of the polynomial. Take a look at how we can use a polynomial kernel to implement kernel SVM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[10  0  0]\n",
      " [ 0  8  0]\n",
      " [ 0  1 11]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      setosa       1.00      1.00      1.00        10\n",
      "  versicolor       0.89      1.00      0.94         8\n",
      "   virginica       1.00      0.92      0.96        12\n",
      "\n",
      "    accuracy                           0.97        30\n",
      "   macro avg       0.96      0.97      0.97        30\n",
      "weighted avg       0.97      0.97      0.97        30\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC  \n",
    "svclassifier = SVC(kernel='poly', degree=8, gamma='auto')  # more higher degree, \n",
    "# more time the system will take. u cn choose any no as degree\n",
    "# gamma is optional. But it gives a FutureWarning. To avoid it , specify\n",
    "# gamma as 'auto' or 'scale'\n",
    "\n",
    "svclassifier.fit(X_train, y_train)\n",
    "\n",
    "# Making Predictions\n",
    "# Now once we have trained the algorithm, \n",
    "# the next step is to make predictions on the test data.\n",
    "y_pred = svclassifier.predict(X_test)  \n",
    "\n",
    "\n",
    "# Evaluating the Algorithm\n",
    "from sklearn.metrics import classification_report, confusion_matrix  \n",
    "print(confusion_matrix(y_test, y_pred))  \n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Note : Note the misclassification in 'virginica' species"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Gaussian Kernel\n",
    "\n",
    "To use Gaussian kernel, you have to specify 'rbf' as value for the Kernel parameter of the SVC class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[10  0  0]\n",
      " [ 0  8  0]\n",
      " [ 0  1 11]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      setosa       1.00      1.00      1.00        10\n",
      "  versicolor       0.89      1.00      0.94         8\n",
      "   virginica       1.00      0.92      0.96        12\n",
      "\n",
      "    accuracy                           0.97        30\n",
      "   macro avg       0.96      0.97      0.97        30\n",
      "weighted avg       0.97      0.97      0.97        30\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC  \n",
    "svclassifier = SVC(kernel='rbf', gamma='auto')  \n",
    "svclassifier.fit(X_train, y_train) \n",
    "\n",
    "# Prediction and Evaluation\n",
    "y_pred = svclassifier.predict(X_test)  \n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix  \n",
    "print(confusion_matrix(y_test, y_pred))  \n",
    "print(classification_report(y_test, y_pred))  \n",
    "\n",
    "# Note : Note the best performance thats 100% precise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Sigmoid Kernel\n",
    "Finally, let's use a sigmoid kernel for implementing Kernel SVM. \n",
    "To use the sigmoid kernel, you have to specify 'sigmoid' as value for the kernel parameter of the SVC class.Take a look at the following script:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  0 13]\n",
      " [ 0  0  9]\n",
      " [ 0  0  8]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      setosa       0.00      0.00      0.00        13\n",
      "  versicolor       0.00      0.00      0.00         9\n",
      "   virginica       0.27      1.00      0.42         8\n",
      "\n",
      "    accuracy                           0.27        30\n",
      "   macro avg       0.09      0.33      0.14        30\n",
      "weighted avg       0.07      0.27      0.11        30\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ELCOT\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC  \n",
    "svclassifier = SVC(kernel='sigmoid', gamma='auto')  \n",
    "svclassifier.fit(X_train, y_train)\n",
    "\n",
    "# Prediction and Evaluation\n",
    "y_pred = svclassifier.predict(X_test)  \n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix  \n",
    "print(confusion_matrix(y_test, y_pred))  \n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Note : Note the very poor perfomance from Sigmoid kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison of Kernel Performance\n",
    "\n",
    "If we compare the performance of the different types of kernels we can clearly see that the sigmoid kernel performs the worst. This is due to the reason that sigmoid function returns two values, 0 and 1, therefore it is more suitable for binary classification problems. However, in our case we had three output classes.\n",
    "\n",
    "Amongst the Gaussian kernel and polynomial kernel, we can see that Gaussian kernel achieved a perfect 100% prediction rate while polynomial kernel misclassified three instances. Therefore the Gaussian kernel performed slightly better. However, there is no hard and fast rule as to which kernel performs best in every scenario. It is all about testing all the kernels and selecting the one with the best results on your test dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
